{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サポートベクター回帰 (Support Vector Regression, SVR)\n",
    "\n",
    "# 概要\n",
    "「サポートベクトル」を用いた「回帰」分析\n",
    "\n",
    "# 特徴\n",
    "* 機械学習の分野で最も広く使われているアルゴリズムの1つ。\n",
    "* 特徴次元数が大きいデータでも高い精度が出る。\n",
    "* 非線形なデータも扱うことができる。\n",
    "\n",
    "# 詳細 パターン１\n",
    "* サポートベクトルマシン\n",
    "    * サポートベクトル＝決定境界から最も近い正のサンプルと負のサンプル\n",
    "    * 正の超平面,負の超平面＝サポートベクトルを通る超平面\n",
    "    * マージン(＝2つの超平面の間隔)2/|w|を最大化する\n",
    "    * 関数L=|w|^2/2を最小化する問題に置き換える\n",
    "* 直線で分けることが出来ない場合(1) ソフトマージン分類\n",
    "    * 境界からはみ出ることを許可する＆ペナルティを与える\n",
    "    * スラック変数ξ＝どれだけはみ出したかの度合\n",
    "    * 関数L=|w|^2/2+C*(Σ_i ξ(i)) を最小化する問題に置き換える\n",
    "    * 実際には、等価な双対問題を導出して2次計画法を用いて最小化問題を解く\n",
    "* 直線で分けることが出来ない場合(2) 非線形SVM\n",
    "    * 変数x→非線形写像Φ(x)でより高い次元に拡張した空間においてSVMを適用する\n",
    "    * RBFカーネルは、実質無限次元への拡張\n",
    "    * 表現力が高い（過学習しやすいとも言う）\n",
    "* サポートベクトル回帰\n",
    "    * 回帰の誤差がεを超えたときにペナルティを与える\n",
    "    * ξやLを同様に考えてSVMを適用する\n",
    "\n",
    "# 説明 パターン２\n",
    "* 線形回帰 f(x)=x*b\n",
    "* 非線形回帰 f(x)=Φ(x)*w+c Φ:非線形写像\n",
    "* SVMとSVRの比較\n",
    "    * SVM「マージンの逆数(＝重み)＋誤分類の数」最小化\n",
    "    * SVR「重み＋誤差」の最小化\n",
    "* SVRの誤差関数：h(y-f(x)) = 「誤差y-f(x)が+/-ε以下なら0、誤差が...以上なら、|誤差|-ε」\n",
    "    ※ある程度の誤差(ε以下)は無視する\n",
    "* スラック変数ξ,ξ*を導入して、以下のように変形\n",
    "    誤差関数 h(y-f(x)) = ξ+ξ*\n",
    "* 以下、最小化するときの定式化は省略\n",
    "    未定乗数法や二次計画問題、カーネルトリックなどを使う\n",
    "* サポートベクター = 誤差がε以上のサンプル\n",
    "* パラメーターの意味\n",
    "    * ε:誤差の許容範囲\n",
    "    * C:モデルの複雑さ\n",
    "    * γ：カーネル\n",
    "\n",
    "# 説明 パターン３\n",
    "* サポートベクターマシンとの類推よりも、線形回帰の一種であるリッジ回帰から考える\n",
    "* リッジ回帰において「誤差の二乗和」としたのを、「『誤差の絶対値 -ε（誤差がε以下なら０）』の和」としたもの = 線形サポートベクトル回帰\n",
    "* 線形サポートベクトル回帰 + カーネルトリック = 非線形も扱えるサポートベクトル回帰\n",
    "\n",
    "\n",
    "# ハイパーパラメータの高速最適化\n",
    "* SVRのハイパーパラメータ\n",
    "    * C の候補: $2^{-5}, 2^{-4}, …, 2^{9}, 2^{10}$ (16通り)\n",
    "    * ε (イプシロン) の候補: $2^{-10}, 2^{-9}, …, 2^{-1}, 2^{0}$ (11通り)\n",
    "    * γ (ガンマ) の候補: $2^{-20}, 2^{-19}, …, 2^{9}, 2^{10}$ (31通り)\n",
    "    * グリッドサーチだと$16 \\times 11 \\times 31=5456$回も必要\n",
    "* 高速で最適化する方法\n",
    "    1. γ = グラム行列の分散が最大になるもの、と仮定\n",
    "    2. C = 目的変数yの平均＋/-標準偏差×3、あたりがいいので、データが標準化済みなら、C=3、と仮定\n",
    "    3. 1と2の、γとCを用いて、εのみを最適化（ε決定）\n",
    "    4. 1と3の、γとεを用いて、Cのみを最適化（C決定）\n",
    "    5. 3と4の、εとCを用いて、γのみを最適化（γ決定）\n",
    "    6. パラメータ１つずつ最適化するので、$11+16+31=56$回で済む\n",
    "* グラム行列とは？\n",
    "    * 前提：カーネルk(x,y)と、入力x1,x2,...,xnがある\n",
    "    * カーネル値 k(xi,xj) を i,j 成分とする行列がグラム行列\n",
    "* ハイパーパラメータの値の候補はどうして2のべき乗・累乗なのか？\n",
    "    1. 探索範囲が広いので、ある程度荒くする\n",
    "    2. $10^n$よりも細かい\n",
    "    3. 細かく最適化しても、予測性能が悪いと意味がない\n",
    "\n",
    "# 誤差が一定のところにサンプルが固まる現象\n",
    "1. SVR (Support Vector Regression, サポートベクター回帰) で回帰モデルを構築したとき  \n",
    "    yy-plot(実測値 vs. 推定値プロット)において、対角線から一定に離れたところにサンプルが固まる  \n",
    "     つまり全く同じ誤差のサンプルがある、 ε チューブの境界に、サンプルが固まる、場合がある\n",
    "2. この現象自体にはまったく問題はない\n",
    "3. SVRの計算式からの証明。詳細は省く。  \n",
    "    モデル構築における最適化の段階で、0 < αj,aj* < C となったサンプルは、誤差がε,-εになる  \n",
    "4. 3から言えること\n",
    "    a. ε チューブの内側のサンプル、つまり誤差が小さいサンプルは、SVR モデルに影響しないため、\n",
    "      SVR モデルはノイズの影響を受けにくい\n",
    "    b. ε チューブの外側のサンプル、つまり誤差が大きいサンプルは、  \n",
    "      どんなに誤差が大きくても、SVR モデルへの影響を C でコントロールできるため、  \n",
    "      SVR モデルは目的変数の外れ値の影響は受けにくい\n",
    "\n",
    "## 自分の解釈\n",
    "* 上記では、誤差がεになるサンプルがどんなものかを説明はしてるけど、そういうサンプルが多い(1.の現象)の説明はしてないような気がする\n",
    "* サンプルが固まるのはεチューブの「境界」というより「内部」では？その場合は、以下の通り理解できる。\n",
    "    * SVRは「誤差ε以上のサンプルをなるべく減らすようにモデルを構築する」\n",
    "    * つまり「ε以下の誤差があっても気にしない(モデル構築には影響しない)」ので、そのモデルで予測した場合、誤差が0~εになるものが多いということになる\n",
    "\n",
    "\n",
    "# 参考文献\n",
    "* サポートベクター回帰(Support Vector Regression, SVR)～サンプル数10000以下ならこれを使うべし！～  \n",
    "    https://datachemeng.com/supportvectorregression/\n",
    "* 第9回 サポートベクターマシンで非線形な競馬予測に挑む　（AlphaImpact）  \n",
    "    https://alphaimpact.jp/2017/03/16/svm/\n",
    "* [Pythonコードあり] サポートベクター回帰(Support Vector Regression, SVR)のハイパーパラメータを高速に最適化する方法  \n",
    "    https://datachemeng.com/fastoptsvrhyperparams/\n",
    "* SVM・SVRのハイパーパラメータの値の候補はどうして2のべき乗・累乗なのか？  \n",
    "    http://univprof.com/archives/16-06-25-4229971.html\n",
    "* あなたはサポートベクターマシン・回帰 (Support Vector Machine or Regression, SVM or SVR)におけるハイパーパラメータの設定に時間がかかっていませんか？  \n",
    "    http://univprof.com/archives/16-07-14-4701508.html\n",
    "* SVR(サポートベクター回帰)で誤差が一定のところにサンプルが固まるのはどうして？何か問題があるの？ → SVR の特徴も確認！  \n",
    "    https://datachemeng.com/svr_epsilon_tube/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
