{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 局所部分的最小二乗回帰(Locally-Weighted Partial Least Squares, LWPLS)\n",
    "\n",
    "# 概要\n",
    "非線形性に対応したＰＬＳ\n",
    "\n",
    "# 詳細\n",
    "* Just-In-Time (JIT) モデリングの一つ\n",
    "* 目的変数の値を推定したいサンプルごとにモデリングする\n",
    "* 目的変数の値を推定したいサンプルに近いトレーニングデータほど大きい重みを付けてモデリングする\n",
    "* 時系列データなど、説明変数と目的変数のデータセットのサンプルが増えていく場合に特に効果を発揮する\n",
    "\n",
    "# 定式化\n",
    "$n$次元のトレーニングデータ$m$個の入力と出力を$X$と$y$、出力を推定したい$n$次元のサンプル（クエリ）1個を$x_q$とする。\n",
    "\n",
    "* トレーニングデータとクエリの類似度$u_q$を計算。\n",
    "    * $u_q: exp(-|x(i)-x_q|^2/λ s(i))$\n",
    "        * $x(i):i$番目のトレーニングデータ\n",
    "        * $s(i):|x(i)-x_q|$の標準偏差\n",
    "        * λ:類似度調整用ハイパーパラメータ\n",
    "    * $U$: 対角成分が$u_q$、他はゼロの行列\n",
    "\n",
    "* トレーニングデータの平均化\n",
    "* $X_0 = X - X_w$\n",
    "    * $X_w =  [1,1,1,...]^T [x_{w,1},x_{w,2},x_{w,3},...,x_{w,n}]$\n",
    "    * $x_{w,j} = \\sum_{i=1}^m u_q(i) x_j(i)/\\sum_{i=1}^m u_q(i)$\n",
    "* $y_0 = y - y_w$\n",
    "    * $y_w = \\sum_{i=1}^m u_q(i) y(i)/\\sum_{i=1}^m u_q(i)$\n",
    "\n",
    "* 出力の第一成分$y_1$について、以下の順番で計算。\n",
    "    * $w_1 = X_0 ^T U y_0 / ||X_0^T y_0||$\n",
    "    * $t_1 = X_0 w_1$\n",
    "    * $p_1 = X_0^T U t_1/t_1^T U t_1$\n",
    "    * $q_1 = y_0^T U t_1/t_1^T U t_1$\n",
    "    * $X_1 = X_0 - t_1 p_1^T$\n",
    "    * $y_1 = y_0 - t_1 q_1$\n",
    "* 出力の第二成分$y_2$についても、上の式で$1→2、0→1$として、同様の計算を行う。\n",
    "* 以降、$y_3,y4,...$についても繰り返し。\n",
    "* ポイント：予測したいサンプル1個 $x_q$を基に$U$で重みを付けている\n",
    "\n",
    "# 補足\n",
    "* 非線形っぽく見えない？\n",
    "* ここは多分共通\n",
    "    * $X =\\sum t(主成分ベクトル)*p(ローディングベクトル)^T + E(残差ベクトル)$\n",
    "    * $y = \\sum t(主成分ベクトル)*q(係数) + f(残差ベクトル)$\n",
    "* PLS:\n",
    "    * $w_1 = X^T y / ||X^T y||$\n",
    "    * $t_1 = X w1$\n",
    "    * $p_1 = X^T t_1/t_1^T t_1$\n",
    "    * $q_1 = y^T t_1/t_1^T t_1$\n",
    "    * $t_1$はXの線形変換、$p_1,q_1$は一定値のベクトル・スカラーなので、yもXに対して線形\n",
    "\n",
    "* LWPLS:\n",
    "    * $w_1 = X_0 ^T U y_0 / ||X_0^T y_0||$\n",
    "    * $t_1 = X_0 w_1$\n",
    "    * $p_1 = X_0^T U t_1/t_1^T U t_1$\n",
    "    * $q_1 = y_0^T U t_1/t_1^T U t_1$\n",
    "    * $U$は予測したいデータ(クエリ)に依存する数値を対角成分に持つ行列\n",
    "* よって、「個々の」推定は線形モデルを元にしているが、その線形モデルの係数はクエリによって変わるため、「全体の」関数は非線形になる\n",
    "\n",
    "* まとめ\n",
    "    * LWPLSは、予測したいデータの近くの学習データをなるべく再現するよう線形モデルを組む\n",
    "    * LWPLSは、小さい区間での線形補完を繰り返すようなもの\n",
    "\n",
    "* 余談\n",
    "    * なぜ一部のＸが大文字？\n",
    "    * 多次元配列(行列)は大文字、 1次元配列(ベクトル)は小文字、という数学的な慣習に従った表記。\n",
    "\n",
    "# 参考文献\n",
    "* Locally-Weighted Partial Least Squares (LWPLS, 局所PLS)～あのPLSが非線形性に対応！～ [Python・MATLABコードあり]  \n",
    "    https://datachemeng.com/locallyweightedpartialleastsquares/\n",
    "* Locally-Weighted Partial Least Squares (LWPLS)  \n",
    "    https://github.com/hkaneko1985/lwpls\n",
    "* Pythonではじめる機械学習――scikit-learnで学ぶ特徴量エンジニアリングと機械学習の基礎\n",
    "    https://www.oreilly.co.jp/books/9784873117980/\n",
    "* scikit-learn - PyQ 1.0 ドキュメント\n",
    "    https://docs.pyq.jp/python/machine_learning/tips/scikit-learn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
