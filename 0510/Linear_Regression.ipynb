{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰 (Linear Regression)\n",
    "\n",
    "# 概要\n",
    "* 線形回帰は、モデルを線形と仮定した回帰分析  \n",
    "* 係数を決めるに当たって、何を基準にするかによって、以下の通り分類される\n",
    "    * 誤差の二乗和が最小になるように = 最小二乗法 (Least Squares)\n",
    "    * 誤差の二乗和 + 回帰係数の二乗和が最小になるように = リッジ回帰 (Ridge Regression)\n",
    "    * 誤差の二乗和 + 回帰係数の絶対値の和が最小になるように = LASSO (least absolute shrinkage and selection operator)\n",
    "    * 誤差の二乗和 + 回帰係数の二乗和 + 回帰係数の絶対値の和が最小になるように = Elastic Net\n",
    "    * 誤差関数 + 回帰係数の二乗和が最小になるように = サポートベクター回帰 (Support Vector Regression, SVR）\n",
    "\n",
    "# 豆知識\n",
    "* LASSOとElastic Netは回帰係数の値が0になりやすく、変数選択としても利用できる  \n",
    "    実例：https://doi.org/10.1103/PhysRevLett.114.105503  \n",
    "* なぜ係数が小さい方が過学習を防ぐのか？\n",
    "    過学習するときには、係数の値が大きくなりがちだから  \n",
    "* Elastic Net = LASSO (単純) + Ridge (複雑)\n",
    "* 最適化するべきパラメータ\n",
    "    * LS:なし\n",
    "    * RR:λ\n",
    "    * LASSO:λ\n",
    "    * EN:λとα\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
